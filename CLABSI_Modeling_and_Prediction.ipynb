{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb1b02b5",
   "metadata": {},
   "source": [
    "# CLABSI_Modeling_and_Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241730bb",
   "metadata": {},
   "source": [
    "Author- Santhosh Botcha , Academic-Business Analysis Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb9d3ff",
   "metadata": {},
   "source": [
    "# Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06439964",
   "metadata": {},
   "source": [
    "**Strategic Data Analysis and Visualization:**\n",
    "\n",
    "Our team's effort in conducting an extensive Exploratory Data Analysis (EDA) was crucial. It enabled us to understand the underlying patterns and relationships within the CLABSI dataset effectively. We identified several factors influencing infection risks, which were instrumental in guiding the development of predictive models.\n",
    "\n",
    "The use of visuals was particularly effective in conveying complex statistical relationships and insights to both the team and potential stakeholders, enhancing the comprehensibility and engagement of our analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fb3ff6",
   "metadata": {},
   "source": [
    "**Data Preprocessing and Feature Engineering:**\n",
    "\n",
    "We strategically selected features based on insights gained from the EDA, focusing on those most relevant for model optimization. This not only improved model accuracy but also streamlined the predictive process by reducing computational demands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b658a4",
   "metadata": {},
   "source": [
    "# Zoom-in Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71ba32d",
   "metadata": {},
   "source": [
    "**Oversampling with imbalanced datasets:**\n",
    "By making the dataset more balanced,oversampling can help improve the model’s sensitivity (true positive rate) for the \n",
    "minority class without needing to collect more data. The number of CLABSI cases (positive class) is much lower compared to non-CLABSI cases (negative class), oversampling the minority class helps balance the dataset. While oversampling can \n",
    "improve model performance in detecting the minority class, it may lead to overfitting, as the model might learn to recognize the oversampled points too well. With CLABSI likely being a relatively rare event, oversampling can help the models to detect more \n",
    "effectively (Addressing imbalance). This technique is particularly useful for logistic regression and _____ algorithms, which can be sensitive to variations in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30faa04d",
   "metadata": {},
   "source": [
    "**Hyperparameter Tuning:**\n",
    "\n",
    "Datasets have unique characteristics, and no single model \n",
    "configuration is likely to be optimal across all datasets or problems. To adjust the \n",
    "regularization strength in logistic regression to prevent overfitting given the potentially \n",
    "high dimensionality of clinical data, we opted Hyperparameter tuning to find the best \n",
    "model settings that maximize performance on given metrics. Our dataset includes a \n",
    "diverse range of features from patient demographics to detailed clinical parameters, the \n",
    "challenge is to build a model that accurately captures important predictors of CLABSI \n",
    "without fitting to noise or peculiarities in your data. We used hyperparameter tuning \n",
    "along with SMOTE for KNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33038971",
   "metadata": {},
   "source": [
    "**Bootstrapping for Model Reliability:**\n",
    "\n",
    "We used this statistical method to improve \n",
    "model’s accuracy and estimate the uncertainty of model estimates by resampling data \n",
    "with replacement. By repeated resampling, we can assess how changes in the data \n",
    "affect our models, thereby giving insight into how stable the model predictions are \n",
    "across different samples and to ensure the findings are not a result of random variation \n",
    "in the data. We used bootstrapping with decision trees algorithm in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1f0eb8",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646d9406",
   "metadata": {},
   "source": [
    "We imported the final dataset (after cleansing and preprocessing) from Assignment 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "079bcb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3bd8311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clabsi_new = pd.read_csv('clabsi_new.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c998f08",
   "metadata": {},
   "source": [
    "**Developed Models**\n",
    "\n",
    "• Logistic Regression\n",
    "\n",
    "• KNN\n",
    "\n",
    "• Decision Trees\n",
    "\n",
    "• Neural Networks\n",
    "\n",
    "• XGBoosing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fea268",
   "metadata": {},
   "source": [
    "### **LOGISTIC REGRESSION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad5e71b",
   "metadata": {},
   "source": [
    "Logistic regression is a fundamental statistical model used for binary classification tasks, where the target variable has two possible outcomes, typically labeled as 0 and 1. Despite its name, logistic regression is a classification algorithm rather than a regression algorithm. It's widely used in various fields such as healthcare, finance, and marketing for predicting outcomes like whether a customer will churn or not, whether a patient has a disease or not, etc.\n",
    "\n",
    "The core idea behind logistic regression is to model the probability of the target variable belonging to a particular class based on one or more predictor variables. It assumes a linear relationship between the predictors and the log odds of the target variable, which is then transformed using the logistic function (sigmoid function) to obtain the predicted probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babce0e6",
   "metadata": {},
   "source": [
    "**Benefits**\n",
    "\n",
    "• Simple and easy to understand\n",
    "\n",
    "• Computationally efficient, especially for large datasets\n",
    "\n",
    "• Provides probabilities for predictions, allowing for a more nuanced interpretation of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc5cb46",
   "metadata": {},
   "source": [
    "**Limitations**\n",
    "\n",
    "• Assumes a linear relationship between the features and the log-odds of the target variable\n",
    "\n",
    "• May not perform well with highly non-linear relationships in the data\n",
    "\n",
    "• Prone to overfitting if the number of features is large relative to the number of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b87a5595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9957865168539326\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      1.00      1.00      2836\n",
      "        True       0.00      0.00      0.00        12\n",
      "\n",
      "    accuracy                           1.00      2848\n",
      "   macro avg       0.50      0.50      0.50      2848\n",
      "weighted avg       0.99      1.00      0.99      2848\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2836    0]\n",
      " [  12    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\malinthika fernando\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\malinthika fernando\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\malinthika fernando\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'clabsi_new' is your DataFrame and you want to one-hot encode all string columns\n",
    "X_train, X_test, y_train, y_test = train_test_split(clabsi_new, clabsi['HasCLABSI'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Concatenate training and test data\n",
    "combined_data = pd.concat([X_train, X_test], axis=0)\n",
    "\n",
    "# One-hot encode\n",
    "combined_data_encoded = pd.get_dummies(combined_data)\n",
    "\n",
    "# Split back into training and test data\n",
    "X_train_encoded = combined_data_encoded[:len(X_train)]\n",
    "X_test_encoded = combined_data_encoded[len(X_train):]\n",
    "\n",
    "# Impute missing values with the mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train_encoded)\n",
    "X_test_imputed = imputer.transform(X_test_encoded)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f78bd2c",
   "metadata": {},
   "source": [
    "The warnings above (UndefinedMetricWarning) are because the precision and F1-score for the 'True' class are undefined when there are no predicted samples for that class. This can happen when the model predicts only one class consistently, as seems to be the case here. So we modified the code as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7d86d4",
   "metadata": {},
   "source": [
    "pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7e6be1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9964887640449438\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      1.00      1.00      2836\n",
      "        True       1.00      0.17      0.29        12\n",
      "\n",
      "    accuracy                           1.00      2848\n",
      "   macro avg       1.00      0.58      0.64      2848\n",
      "weighted avg       1.00      1.00      1.00      2848\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2836    0]\n",
      " [  10    2]]\n",
      "AUC: 0.5833333333333334\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Assuming 'clabsi_new' is your DataFrame and you want to one-hot encode all string columns\n",
    "X_train, X_test, y_train, y_test = train_test_split(clabsi_new, clabsi['HasCLABSI'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Concatenate training and test data\n",
    "combined_data = pd.concat([X_train, X_test], axis=0)\n",
    "\n",
    "# One-hot encode\n",
    "combined_data_encoded = pd.get_dummies(combined_data)\n",
    "\n",
    "# Split back into training and test data\n",
    "X_train_encoded = combined_data_encoded[:len(X_train)]\n",
    "X_test_encoded = combined_data_encoded[len(X_train):]\n",
    "\n",
    "# Impute missing values with the mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train_encoded)\n",
    "X_test_imputed = imputer.transform(X_test_encoded)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Apply oversampling\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Fit the model on the resampled training data\n",
    "log_reg.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "print(f\"AUC: {auc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e8ee1a",
   "metadata": {},
   "source": [
    "This output is the result of evaluating your logistic regression model after applying oversampling to address the imbalanced class issue. Here's a breakdown of the output:\n",
    "\n",
    "1. **Accuracy:** 0.9965 means that the model correctly predicted the outcome for approximately 99.7% of the test samples. This is a high accuracy rate, indicating that the model performs well overall.\n",
    "\n",
    "2. **Classification Report:** \n",
    "   - For the 'False' class (no CLABSI), precision, recall, and F1-score are all 1.00, indicating that the model correctly predicted all instances of this class.\n",
    "   - For the 'True' class (CLABSI present), precision is 1.00, recall is 0.17, and F1-score is 0.29. This means that while the model is very precise in predicting the 'True' class (when it predicts 'True', it is almost always correct), it misses many instances of the 'True' class (low recall), resulting in a low F1-score. The low recall indicates that the model struggles to correctly identify instances of the 'True' class.\n",
    "\n",
    "3. **Confusion Matrix:** \n",
    "   - The confusion matrix shows that the model correctly predicted 2836 instances of the 'False' class (true negatives) and 2 instances of the 'True' class (true positives). However, it incorrectly predicted 10 instances of the 'False' class as 'True' (false positives).\n",
    "\n",
    "Overall, while the model performs very well in predicting the majority class ('False'), it struggles with the minority class ('True'). This imbalance in performance between the two classes is reflected in the low recall and F1-score for the 'True' class. AUC has a value of 0.5 indicating that the test is no better than chance at distinguishing between diseased and nondiseased individuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4856e5fa",
   "metadata": {},
   "source": [
    "### **KNN MODEL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c59620",
   "metadata": {},
   "source": [
    "The k-Nearest Neighbors (KNN) algorithm is a versatile and intuitive machine learning model used for classification and regression tasks. KNN is a non-parametric, lazy learning algorithm, meaning it doesn't make assumptions about the underlying data distribution and defers processing until a prediction is needed. This makes KNN straightforward to implement and understand, making it a popular choice for beginners and as a baseline model for comparison in more complex tasks.\n",
    "\n",
    "In KNN, the prediction for a new data point is determined by the majority class (in classification) or the mean value (in regression) of its k nearest neighbors in the feature space. The \"k\" in KNN represents the number of neighbors to consider, and it's a hyperparameter that needs to be tuned based on the dataset and problem at hand. Generally, odd values of k are chosen to avoid ties in classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc95e1dc",
   "metadata": {},
   "source": [
    "**Benefits**\n",
    "\n",
    "• Simple and easy to implement\n",
    "\n",
    "• No training phase, which makes the training process very fast\n",
    "\n",
    "• Versatile, as it can be used for classification and regression tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d217136d",
   "metadata": {},
   "source": [
    "**Limitations**\n",
    "\n",
    "• Computationally expensive, especially when dealing with large datasets\n",
    "\n",
    "• Sensitivity to irrelevant features and the choice of distance metric\n",
    "\n",
    "• Requires a meaningful distance metric for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "87757cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy: 0.9957865168539326\n",
      "KNN Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      1.00      1.00      2836\n",
      "        True       0.00      0.00      0.00        12\n",
      "\n",
      "    accuracy                           1.00      2848\n",
      "   macro avg       0.50      0.50      0.50      2848\n",
      "weighted avg       0.99      1.00      0.99      2848\n",
      "\n",
      "KNN Confusion Matrix:\n",
      "[[2836    0]\n",
      " [  12    0]]\n",
      "AUC: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\malinthika fernando\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\malinthika fernando\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\malinthika fernando\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Initialize the KNN classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Fit the model on the resampled training data\n",
    "knn.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_knn = knn.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "print(f\"KNN Accuracy: {accuracy_knn}\")\n",
    "\n",
    "# Print classification report for KNN\n",
    "print(\"KNN Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "\n",
    "# Print confusion matrix for KNN\n",
    "print(\"KNN Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_knn))\n",
    "\n",
    "auc = roc_auc_score(y_test, y_pred_knn)\n",
    "print(f\"AUC: {auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422ed53f",
   "metadata": {},
   "source": [
    "The UndefinedMetricWarning messages indicate that precision and F1-score are undefined for the 'True' class because there were no predicted samples for that class, which is why they are set to 0.0. This suggests that the KNN model struggles to correctly identify instances of the 'True' class, possibly due to the imbalance in class distribution or the nature of the data.\n",
    "We tried using the class_weight='balanced' parameter in the KNN classifier. This parameter automatically adjusts the weights of the classes inversely proportional to their frequencies, which can help address the imbalance issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4012d982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy with balanced class weights: 0.9957865168539326\n",
      "KNN Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      1.00      1.00      2836\n",
      "        True       0.00      0.00      0.00        12\n",
      "\n",
      "    accuracy                           1.00      2848\n",
      "   macro avg       0.50      0.50      0.50      2848\n",
      "weighted avg       0.99      1.00      0.99      2848\n",
      "\n",
      "KNN Confusion Matrix:\n",
      "[[2836    0]\n",
      " [  12    0]]\n",
      "AUC: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\malinthika fernando\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\malinthika fernando\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\malinthika fernando\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Initialize the KNN classifier with class_weight='balanced'\n",
    "knn = KNeighborsClassifier(weights='distance')\n",
    "\n",
    "# Fit the model on the resampled training data\n",
    "knn.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_knn = knn.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "print(f\"KNN Accuracy with balanced class weights: {accuracy_knn}\")\n",
    "\n",
    "# Print classification report for KNN\n",
    "print(\"KNN Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "\n",
    "# Print confusion matrix for KNN\n",
    "print(\"KNN Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_knn))\n",
    "\n",
    "auc = roc_auc_score(y_test, y_pred_knn)\n",
    "print(f\"AUC: {auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d372c048",
   "metadata": {},
   "source": [
    "Overall, the class_weight='balanced' parameter did not significantly improve the model's ability to correctly predict the minority class. We needed to try other approaches to address the imbalance issue and improve the model's performance on the minority class.\n",
    "So we tried another method to improve the model's performance - using the Synthetic Minority Over-sampling Technique (SMOTE) to generate synthetic samples for the minority class. SMOTE works by creating synthetic samples that are similar to existing samples in the minority class, thereby balancing the class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9651daf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy with SMOTE: 0.1931179775280899\n",
      "KNN Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.19      0.32      2836\n",
      "        True       0.01      1.00      0.01        12\n",
      "\n",
      "    accuracy                           0.19      2848\n",
      "   macro avg       0.50      0.59      0.16      2848\n",
      "weighted avg       1.00      0.19      0.32      2848\n",
      "\n",
      "KNN Confusion Matrix:\n",
      "[[ 538 2298]\n",
      " [   0   12]]\n",
      "AUC: 0.594851904090268\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Assuming 'clabsi_new' is your DataFrame and you want to one-hot encode all string columns\n",
    "X_train, X_test, y_train, y_test = train_test_split(clabsi_new, clabsi['HasCLABSI'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Concatenate training and test data\n",
    "combined_data = pd.concat([X_train, X_test], axis=0)\n",
    "\n",
    "# One-hot encode\n",
    "combined_data_encoded = pd.get_dummies(combined_data)\n",
    "\n",
    "# Split back into training and test data\n",
    "X_train_encoded = combined_data_encoded[:len(X_train)]\n",
    "X_test_encoded = combined_data_encoded[len(X_train):]\n",
    "\n",
    "# Impute missing values with the mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train_encoded)\n",
    "X_test_imputed = imputer.transform(X_test_encoded)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Apply SMOTE to resample the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Initialize the KNN classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Fit the model on the resampled training data\n",
    "knn.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_knn = knn.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "print(f\"KNN Accuracy with SMOTE: {accuracy_knn}\")\n",
    "\n",
    "# Print classification report for KNN\n",
    "print(\"KNN Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "\n",
    "# Print confusion matrix for KNN\n",
    "print(\"KNN Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_knn))\n",
    "\n",
    "auc = roc_auc_score(y_test, y_pred_knn)\n",
    "print(f\"AUC: {auc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2131ffe9",
   "metadata": {},
   "source": [
    "The output above indicates that our KNN model with SMOTE oversampling has an accuracy of approximately 19.31%. However, the precision, recall, and F1-score for the minority class (True) are very low, indicating poor performance in identifying positive cases.\n",
    "\n",
    "Precision: The proportion of correctly identified positive cases out of all predicted positive cases is extremely low for the minority class (True), indicating that the model is incorrectly classifying many negative cases as positive.\n",
    "\n",
    "Recall: The proportion of correctly identified positive cases out of all actual positive cases is 100% for the minority class (True), indicating that the model is able to correctly identify all positive cases but at the cost of incorrectly labeling many negative cases as positive.\n",
    "\n",
    "F1-score: The harmonic mean of precision and recall for the minority class (True) is very low, indicating overall poor performance in correctly identifying positive cases while minimizing false positives.\n",
    "\n",
    "The confusion matrix shows that the model correctly identified all 12 positive cases (True) but misclassified a large number of negative cases (False) as positive, leading to the low precision and high recall for the minority class.\n",
    "\n",
    "This suggests that while the model is able to identify all positive cases, it does so at the cost of a high number of false positives. Improving the model's performance may require further tuning of hyperparameters, feature selection, or trying different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bd0dcfb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clabsi_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9932123a2a77>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Assuming 'clabsi_new' is your DataFrame and you want to one-hot encode all string columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclabsi_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclabsi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'HasCLABSI'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Concatenate training and test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clabsi_new' is not defined"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Assuming 'clabsi_new' is your DataFrame and you want to one-hot encode all string columns\n",
    "X_train, X_test, y_train, y_test = train_test_split(clabsi_new, clabsi['HasCLABSI'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Concatenate training and test data\n",
    "combined_data = pd.concat([X_train, X_test], axis=0)\n",
    "\n",
    "# One-hot encode\n",
    "combined_data_encoded = pd.get_dummies(combined_data)\n",
    "\n",
    "# Split back into training and test data\n",
    "X_train_encoded = combined_data_encoded[:len(X_train)]\n",
    "X_test_encoded = combined_data_encoded[len(X_train):]\n",
    "\n",
    "# Impute missing values with the mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train_encoded)\n",
    "X_test_imputed = imputer.transform(X_test_encoded)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Apply SMOTE to resample the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Select top k features\n",
    "selector = SelectKBest(score_func=f_classif, k=10)\n",
    "X_train_selected = selector.fit_transform(X_train_resampled, y_train_resampled)\n",
    "X_test_selected = selector.transform(X_test_scaled)\n",
    "\n",
    "# Initialize the KNN classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Define hyperparameters for tuning\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "# Perform grid search cross-validation for hyperparameter tuning\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Fit the model on the best hyperparameters\n",
    "best_knn = KNeighborsClassifier(**best_params)\n",
    "best_knn.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_knn = best_knn.predict(X_test_selected)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "print(f\"KNN Accuracy with SMOTE: {accuracy_knn}\")\n",
    "\n",
    "# Print classification report for KNN\n",
    "print(\"KNN Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "\n",
    "# Print confusion matrix for KNN\n",
    "print(\"KNN Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_knn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7314c028",
   "metadata": {},
   "source": [
    "The warning message indicates that some features in our dataset are constant, which means they have the same value across all samples. This can cause issues with certain machine learning algorithms, as they may not be able to learn from these features.\n",
    "\n",
    "To address this warning, we removed the constant features before training our model. Here's how we modified our code to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a77b38c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy with SMOTE: 0.9554073033707865\n",
      "KNN Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.96      0.98      2836\n",
      "        True       0.08      0.92      0.15        12\n",
      "\n",
      "    accuracy                           0.96      2848\n",
      "   macro avg       0.54      0.94      0.56      2848\n",
      "weighted avg       1.00      0.96      0.97      2848\n",
      "\n",
      "KNN Confusion Matrix:\n",
      "[[2710  126]\n",
      " [   1   11]]\n",
      "AUC: 0.9361189468735307\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Identify constant features\n",
    "constant_filter = VarianceThreshold(threshold=0)\n",
    "constant_filter.fit(X_train_imputed)\n",
    "\n",
    "# Get non-constant feature indices\n",
    "non_constant_indices = constant_filter.get_support(indices=True)\n",
    "\n",
    "# Filter the training and test data to include only non-constant features\n",
    "X_train_non_constant = X_train_imputed[:, non_constant_indices]\n",
    "X_test_non_constant = X_test_imputed[:, non_constant_indices]\n",
    "\n",
    "# Apply SMOTE to resample the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_non_constant, y_train)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test_non_constant)\n",
    "\n",
    "# Initialize the KNN classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Fit the model on the resampled training data\n",
    "knn.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_knn = knn.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "print(f\"KNN Accuracy with SMOTE: {accuracy_knn}\")\n",
    "\n",
    "# Print classification report for KNN\n",
    "print(\"KNN Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "\n",
    "# Print confusion matrix for KNN\n",
    "print(\"KNN Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_knn))\n",
    "\n",
    "# Assuming y_test and y_pred_knn are your true labels and predicted probabilities, respectively\n",
    "auc = roc_auc_score(y_test, y_pred_knn)\n",
    "print(f\"AUC: {auc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab417a1d",
   "metadata": {},
   "source": [
    "The output we received is from evaluating the KNN model after performing SMOTE resampling and removing constant features. Here's an explanation of the key metrics:\n",
    "\n",
    "1. **Accuracy**: Our model achieved an accuracy of approximately 95.54%. This means that it correctly classified about 95.54% of the samples in the test set.\n",
    "\n",
    "2. **Precision**: Precision is the ratio of true positive predictions to the total number of positive predictions made by the model. For the positive class (True), the precision is quite low at approximately 8%. This indicates that when the model predicts a positive outcome, it is correct only about 8% of the time.\n",
    "\n",
    "3. **Recall (Sensitivity)**: Recall is the ratio of true positive predictions to the total number of actual positive instances. The model has a high recall of approximately 92% for the positive class, indicating that it correctly identifies about 92% of the actual positive instances.\n",
    "\n",
    "4. **F1-Score**: The F1-score is the harmonic mean of precision and recall. It provides a balance between precision and recall. For the positive class, the F1-score is approximately 0.15, which is low. This suggests that there is room for improvement in balancing precision and recall for the positive class.\n",
    "\n",
    "5. **Support**: Support is the number of actual occurrences of the class in the specified dataset. For the positive class, there are only 12 instances in the test set.\n",
    "\n",
    "6. **Confusion Matrix**: The confusion matrix provides a summary of the predictions made by the model. It shows that the model correctly predicted 2710 instances of the negative class (False) and 11 instances of the positive class (True). However, it incorrectly classified 126 instances of the negative class and missed 1 instance of the positive class.\n",
    "\n",
    "Overall, the model shows high accuracy and recall for the positive class but has low precision. This indicates that while the model is good at identifying positive cases, it also has a high false positive rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cc7bec",
   "metadata": {},
   "source": [
    "### **DECISION TREES**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c323ea",
   "metadata": {},
   "source": [
    "Decision trees are versatile and powerful machine learning models used for both classification and regression tasks. They are particularly well-suited for tasks where interpretability and understanding of the decision-making process are important. The fundamental concept behind decision trees is to split the data based on feature values into subsets that are as homogenous as possible with respect to the target variable.\n",
    "\n",
    "In a decision tree, each internal node represents a decision based on a feature, and each branch represents the outcome of that decision leading to a leaf node, which corresponds to a predicted class (in classification) or a predicted value (in regression). The decision-making process starts at the root node and follows the branches based on feature values until a leaf node is reached, providing a straightforward and interpretable path to the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13387f31",
   "metadata": {},
   "source": [
    "**Benefits**\n",
    "\n",
    "• Interpretability: Decision trees are easy to interpret and visualize. You can easily understand how decisions are made at each node of the tree, making them particularly useful for explaining the logic behind predictions to stakeholders or non-technical users.\n",
    "\n",
    "• Handling Non-Linearity: Decision trees can handle non-linear relationships between features and the target variable. They can capture complex decision boundaries without requiring feature engineering to transform data into a linear form.\n",
    "\n",
    "• Handles Both Numerical and Categorical Data: Decision trees can handle both numerical and categorical data without requiring additional preprocessing such as one-hot encoding for categorical variables.\n",
    "\n",
    "• Feature Importance: Decision trees provide a measure of feature importance, indicating which features are most influential in making decisions within the model. This can help in feature selection and understanding the impact of different variables on predictions.\n",
    "\n",
    "• Robust to Outliers: Decision trees are robust to outliers and can handle data with mixed types of distributions without significantly affecting their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89730a9",
   "metadata": {},
   "source": [
    "**Limitations**\n",
    "\n",
    "• Overfitting: Decision trees are prone to overfitting, especially with deep trees and complex datasets. They may learn intricate details of the training data, including noise, which can lead to poor generalization on unseen data.\n",
    "\n",
    "• High Variance: Decision trees have high variance, meaning small changes in the training data can lead to different tree structures and potentially different predictions. This can make them unstable compared to other models like ensemble methods.\n",
    "\n",
    "• Limited Expressiveness: While decision trees can capture complex decision boundaries, they may struggle with capturing relationships that require combining multiple features or considering interactions between features.\n",
    "\n",
    "• Bias Toward Dominant Classes: In classification tasks with imbalanced classes, decision trees may have a bias toward predicting the majority class, especially when not properly balanced or weighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ca1f1f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9998240675580577\n",
      "Best cross-validation score: 1.0\n",
      "Best model: BaggingClassifier(random_state=42)\n",
      "Precision: 0.9996465182043125\n",
      "Recall: 1.0\n",
      "F1 Score: 0.9998232278592893\n",
      "AUC: 0.9998249299719888\n",
      "Confusion Matrix:\n",
      "[[2855    1]\n",
      " [   0 2828]]\n"
     ]
    }
   ],
   "source": [
    "# Function to filter integer and float columns\n",
    "def filter_numeric_columns(X):\n",
    "    return X.select_dtypes(include=[np.number])\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Load dataset (you can replace this with your dataset)\n",
    "\n",
    "X = filter_numeric_columns(clabsi_new.copy())\n",
    "\n",
    "y = clabsi_new['HasCLABSI'].copy()\n",
    "\n",
    "# Bootstrapping for class balancing\n",
    "X_resampled, y_resampled = resample(X[y == 1], y[y == 1], n_samples=X[y == 0].shape[0], random_state=42)\n",
    "X = np.vstack((X, X_resampled))\n",
    "y = np.hstack((y, y_resampled))\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fill nulls with median\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "#idefining Decesion Tree\n",
    "tree = DecisionTreeClassifier()\n",
    "# Initialize a BaggingClassifier\n",
    "bagging_clf = BaggingClassifier(base_estimator = tree,random_state=42)\n",
    "\n",
    "# Perform cross-validation to find the best model\n",
    "cv_scores = cross_val_score(bagging_clf, X_train, y_train, cv=5)\n",
    "\n",
    "best_score_index = np.argmax(cv_scores)\n",
    "best_score = cv_scores[best_score_index]\n",
    "best_model = BaggingClassifier(random_state=42)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set using the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Best cross-validation score:\", best_score)\n",
    "print(\"Best model:\", best_model)\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, y_pred)\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, y_pred)\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "# Calculate AUC\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "print(f\"AUC: {auc}\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26924f79",
   "metadata": {},
   "source": [
    "**Accuracy**: The model achieved an exceptional accuracy score of approximately 99.98%, indicating that it correctly classified the vast majority of samples in the test set.\n",
    "\n",
    "**Precision**: Precision, which measures the accuracy of positive predictions, is remarkably high at approximately 99.96%. This indicates that when the model predicts a positive outcome, it is correct nearly 100% of the time.\n",
    "\n",
    "**Recall (Sensitivity)**: The model exhibits perfect recall of 100% for the positive class, suggesting that it correctly identifies all actual positive instances without any false negatives.\n",
    "\n",
    "**F1-Score**: The F1-score, representing the balance between precision and recall, is nearly 99.98% for the positive class. This high value indicates an excellent balance between precision and recall.\n",
    "\n",
    "**AUC (Area Under the Curve)**:The AUC score, which measures the model's ability to distinguish between positive and negative classes, is approximately 99.98%. This indicates outstanding performance in class separation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c51523",
   "metadata": {},
   "source": [
    "### **NEURAL NETWORKS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875f1c8a",
   "metadata": {},
   "source": [
    "Neural networks (NNs) are sophisticated machine learning models inspired by the structure and functioning of the human brain's neural networks. They are highly versatile and widely used across various domains, including computer vision, natural language processing, and time series analysis, among others. NNs are particularly renowned for their ability to learn complex patterns and relationships in data, making them a cornerstone of deep learning.\n",
    "\n",
    "At their core, neural networks consist of interconnected layers of artificial neurons, each performing computations on input data and passing the results to the next layer. The neurons within a layer are organized into multiple hidden layers between the input and output layers, allowing the network to learn hierarchical representations of the data. The connections between neurons are associated with weights that are adjusted during training to minimize the error between predicted and actual outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5c0cc0",
   "metadata": {},
   "source": [
    "**Benefits**\n",
    "\n",
    "• Complex Pattern Recognition: Neural networks excel at learning complex patterns and relationships in data, making them suitable for tasks such as image recognition, natural language processing, and speech recognition.\n",
    "\n",
    "• Feature Learning: NNs can automatically learn relevant features from raw data, reducing the need for manual feature engineering and making them effective for tasks with high-dimensional data.\n",
    "\n",
    "• Non-Linearity: Neural networks can model non-linear relationships between input features and the target variable, allowing them to capture intricate structures in the data that linear models may miss.\n",
    "\n",
    "• Scalability: NNs can scale well to large datasets and computational resources, especially when using deep learning architectures like convolutional neural networks (CNNs) or recurrent neural networks (RNNs).\n",
    "\n",
    "• Parallel Processing: Neural networks can leverage parallel processing capabilities of modern hardware, such as GPUs and TPUs, to accelerate training and inference tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ab6801",
   "metadata": {},
   "source": [
    "**Limitations**\n",
    "\n",
    "• Complexity: Neural networks are complex models with many parameters and hyperparameters to tune. This complexity can make them challenging to interpret and debug, requiring specialized knowledge and expertise.\n",
    "\n",
    "• Large Data Requirements: NNs typically require large amounts of data for training to generalize well and avoid overfitting. Insufficient data can lead to poor performance and generalization on unseen data.\n",
    "\n",
    "• Computational Resources: Training deep neural networks can be computationally intensive, requiring significant computational resources, memory, and time, especially for deep architectures and large datasets.\n",
    "\n",
    "• Overfitting: Neural networks are susceptible to overfitting, especially with limited data or overly complex architectures. Regularization techniques and careful hyperparameter tuning are necessary to mitigate overfitting.\n",
    "\n",
    "• Black Box Nature: Despite their impressive performance, NNs are often seen as \"black box\" models, meaning it can be challenging to understand and interpret how they arrive at their predictions, particularly for deep architectures with many layers.\n",
    "\n",
    "• Hyperparameter Sensitivity: Neural networks are sensitive to hyperparameter choices, such as learning rate, batch size, and network architecture. Finding optimal hyperparameters can be time-consuming and require extensive experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "931f3977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "711/711 [==============================] - 1s 751us/step - loss: 32.0908 - accuracy: 0.6181\n",
      "Epoch 2/15\n",
      "711/711 [==============================] - 0s 601us/step - loss: 1.8903 - accuracy: 0.7851\n",
      "Epoch 3/15\n",
      "711/711 [==============================] - 0s 622us/step - loss: 0.9615 - accuracy: 0.8570\n",
      "Epoch 4/15\n",
      "711/711 [==============================] - 0s 600us/step - loss: 0.5431 - accuracy: 0.9029\n",
      "Epoch 5/15\n",
      "711/711 [==============================] - 0s 604us/step - loss: 0.3693 - accuracy: 0.9319\n",
      "Epoch 6/15\n",
      "711/711 [==============================] - 0s 604us/step - loss: 0.2361 - accuracy: 0.9529\n",
      "Epoch 7/15\n",
      "711/711 [==============================] - 0s 606us/step - loss: 0.1801 - accuracy: 0.9630\n",
      "Epoch 8/15\n",
      "711/711 [==============================] - 0s 644us/step - loss: 0.1403 - accuracy: 0.9705\n",
      "Epoch 9/15\n",
      "711/711 [==============================] - 0s 629us/step - loss: 0.1161 - accuracy: 0.9757\n",
      "Epoch 10/15\n",
      "711/711 [==============================] - 0s 643us/step - loss: 2.5331 - accuracy: 0.9221\n",
      "Epoch 11/15\n",
      "711/711 [==============================] - 0s 646us/step - loss: 0.2442 - accuracy: 0.9728\n",
      "Epoch 12/15\n",
      "711/711 [==============================] - 0s 665us/step - loss: 0.0816 - accuracy: 0.9843\n",
      "Epoch 13/15\n",
      "711/711 [==============================] - 0s 619us/step - loss: 0.0576 - accuracy: 0.9885\n",
      "Epoch 14/15\n",
      "711/711 [==============================] - 0s 647us/step - loss: 0.0478 - accuracy: 0.9906\n",
      "Epoch 15/15\n",
      "711/711 [==============================] - 0s 647us/step - loss: 0.1469 - accuracy: 0.9854\n",
      "Precision score: 0.9698216735253772\n",
      "ROC_AUC score: 0.9845938375350141\n",
      "Recall: 1.0\n",
      "AUC: 0.9845938375350141\n",
      "Confusion Matrix:\n",
      "[[2768   88]\n",
      " [   0 2828]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import recall_score, roc_curve, precision_score, f1_score\n",
    "\n",
    "weights_assigned = {0.0:1, 1:250}\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first dense layer with 20 units, ReLU activation, and he_uniform kernel initializer\n",
    "model.add(Dense(20, input_dim=X_train.shape[1], activation='relu', kernel_initializer='he_uniform'))\n",
    "\n",
    "# Add the output layer with 1 unit and sigmoid activation\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, class_weight = weights_assigned, epochs=15)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.round(y_pred)\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print('Precision score:', precision)\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "print('ROC_AUC score:', roc_auc)\n",
    "\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print('Recall:', recall)\n",
    "\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "print(f\"AUC: {auc}\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680d6a06",
   "metadata": {},
   "source": [
    "**Precision**: Our model's precision, sitting at approximately 97.45%, indicates its high accuracy when it predicts a CLABSI event. When the model predicts CLABSI, it is correct roughly 97.45% of the time, which is particularly critical for reducing the incidence of false positives in a clinical setting.\n",
    "\n",
    "**AUC (Area Under the Curve)**: The AUC measures the entire two-dimensional area underneath the entire ROC curve from (0,0) to (1,1). The AUC score of 97.57% indicates that the model effectively differentiates between positive (having CLABSI) and negative (not having CLABSI) cases.\n",
    "\n",
    "**Recall (Sensitivity)**: The recall is 100% for our model, denoting that it successfully identifies all patients who are actual cases of CLABSI. This high sensitivity is essential in medical diagnostics to avoid missing any true cases that require intervention.\n",
    "\n",
    "**F1-Score**: Based on the high precision and recall, the F1-score can be inferred to be quite high, reflecting the model's balanced performance in precision and recall. This balance is important for maintaining a model that neither overlooks true cases nor raises unnecessary alarms.\n",
    "\n",
    "**Support**: In the context of the provided confusion matrix, the support for our model consists of 2717 instances for the negative class (no CLABSI) and 2828 instances for the positive class (has CLABSI).\n",
    "\n",
    "**Confusion Matrix**: The confusion matrix summarizes the performance of the model. It reveals that the model correctly predicted 2717 non-CLABSI cases and 2828 CLABSI cases, with no false negatives (FN), which is pivotal for patient safety. However, there are 139 instances where the model predicted CLABSI where there was none (false positives), indicating potential areas for improvement to reduce unnecessary treatments.\n",
    "\n",
    "**Considerations**: Our model performs exceptionally well in terms of sensitivity, which is critical for CLABSI prediction, as missing a true case could be detrimental. However, the presence of false positives indicates that there could be some overfitting or that the model might be too sensitive, capturing too many potential but non-actual CLABSI cases.\n",
    "\n",
    "Overall, the model displays excellent sensitivity and precision in identifying CLABSI cases. However, the number of false positives suggests that while the model is quite adept at flagging CLABSI events, it may also overclassify some instances as CLABSI, leading to possible over-treatment. This suggests that while the model excels at identifying actual cases of CLABSI, there is a possibility to refine the model to lower the incidence of false positive predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90405be",
   "metadata": {},
   "source": [
    "### **XGBOOSTING MODEL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed55bf25",
   "metadata": {},
   "source": [
    "XGBoost (Extreme Gradient Boosting) is a powerful ensemble learning technique that has gained significant popularity in the machine learning community due to its high predictive accuracy and efficiency. It belongs to the family of gradient boosting algorithms, which are designed to combine the predictions of multiple weak learners (often decision trees) to create a strong predictive model. XGBoost specifically focuses on optimizing the gradient boosting process for improved performance and scalability.\n",
    "\n",
    "XGBoost builds a series of decision trees sequentially, with each new tree learning from the errors (residuals) of the previous trees. It uses a gradient descent optimization technique to minimize a loss function, such as mean squared error for regression or log loss for classification, during the training process. This iterative approach allows XGBoost to continuously improve and refine its predictions, leading to higher accuracy compared to individual decision trees.\n",
    "\n",
    "XGBoost provides several hyperparameters that can be tuned to optimize model performance and prevent overfitting. Common hyperparameters include the learning rate, tree depth, number of trees (boosting rounds), regularization parameters (such as lambda and alpha), and subsampling parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12451d24",
   "metadata": {},
   "source": [
    "**Benefits**\n",
    "\n",
    "• High Performance: XGBoost is known for its speed and performance compared to traditional machine learning algorithms.​\n",
    "\n",
    "• Scalability: Handles large datasets efficiently due to parallel processing capabilities.​\n",
    "\n",
    "• Regularization: Built-in regularization techniques prevent overfitting and improve generalization.​\n",
    "\n",
    "• Feature Importance: Provides insights into feature importance, aiding in feature selection and understanding model behavior.​\n",
    "\n",
    "• Flexibility: Supports various objectives and evaluation criteria, making it adaptable to different tasks.​"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10ee3e9",
   "metadata": {},
   "source": [
    "**Limitations**\n",
    "\n",
    "• Complexity: Requires understanding of hyperparameters and tuning for optimal performance.​\n",
    "\n",
    "• Computational Resources: Can be resource-intensive, especially for large datasets and complex models.​\n",
    "\n",
    "• Black Box Nature: Interpretability may be challenging due to the ensemble nature of boosting and complex decision trees.​\n",
    "\n",
    "• Data Preparation: Requires preprocessed and cleaned data, may not handle missing values well without preprocessing.​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e345a7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\malinthika fernando\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:13:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 0.9729634831460674\n",
      "Confusion Matrix:\n",
      "[[2770   68]\n",
      " [   9    1]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.98      0.99      2838\n",
      "        True       0.01      0.10      0.03        10\n",
      "\n",
      "    accuracy                           0.97      2848\n",
      "   macro avg       0.51      0.54      0.51      2848\n",
      "weighted avg       0.99      0.97      0.98      2848\n",
      "\n",
      "AUC: 0.66\n"
     ]
    }
   ],
   "source": [
    "#Importing Libraries\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Separating the features (X) and the target variable (y)\n",
    "\n",
    "X = clabsi_new.drop('HasCLABSI', axis=1)\n",
    "y = clabsi_new['HasCLABSI']\n",
    "\n",
    "#Identifying and encoding categorical columns using OneHotEncoder within a preprocessing pipeline\n",
    "\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "#Splitting the data into training and testing sets, with a test size of 20%\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "#Fitting the Preprocessor to Training Data \n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "#Using Synthetic Minority Over-sampling Technique (SMOTE) to handle class imbalance by oversampling the minority class\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train_processed, y_train)\n",
    "\n",
    "#Creating and training an XGBoost classifier model with hyperparameters set for optimal performance\n",
    "\n",
    "best_xgb_model = XGBClassifier(n_estimators=200, max_depth=5, learning_rate=0.1, random_state=42, scale_pos_weight=10)\n",
    "best_xgb_model.fit(X_resampled, y_resampled)\n",
    "\n",
    "#Making predictions on the test set and evaluating the model's performance using accuracy, confusion matrix, classification report, and Area Under the Curve (AUC) score\n",
    "\n",
    "threshold = 0.3  # Adjust this threshold based on your preference\n",
    "y_pred_proba = best_xgb_model.predict_proba(X_test_processed)[:, 1]\n",
    "y_pred = (y_pred_proba > threshold).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f'AUC: {auc_score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f5b19f",
   "metadata": {},
   "source": [
    "**Accuracy**: 97% which means the model classified 97% of the samples correctly\n",
    "\n",
    "**Confusion Matrix:** The model correctly predicted 2770 instances of the negative class and 1 instance of the positive class. However, it incorrectly classified 68 instances of the negative class and missed 9 instance of the positive class.\n",
    "\n",
    "**Classification Report:**\n",
    "\n",
    "Precision: How many of the predicted positives were actually positive. This is low at 1.4%\n",
    "\n",
    "Recall: How many of the actual positives were identified by the model. This is 10% , which means the model captured only 10% of the actual positive cases.\n",
    "\n",
    "F1-Score: The harmonic mean of precision and recall, which is 0.03 in this case.\n",
    "Support: The number of actual occurrences of the class in the dataset. There are 10 positive instances.\n",
    "\n",
    "**Overall**: The model shows high accuracy but has low precision and recall for the positive class. This indicates that while the model is not good at identifying positive cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d5e4fe",
   "metadata": {},
   "source": [
    "# Evaluation Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7edc76e",
   "metadata": {},
   "source": [
    "•\tEvaluation of the overfitting issue\n",
    "\n",
    "•\tEvaluation of the imbalanced outcome issue\n",
    "\n",
    "•\tEvaluation of the classification error\n",
    "\n",
    "•\tEvaluation of the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57282d5",
   "metadata": {},
   "source": [
    "### **Evaluation of the overfitting issue**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092e53f9",
   "metadata": {},
   "source": [
    "In our research on machine learning models, achieving robust generalizability is a primary focus. In a situation where a model becomes overly reliant on the training data's intricacies, it can hinder this goal. This occurs when the model recognizes noise or erratic fluctuations within the training data, leading to poor performance on unseen data. Therefore, effectively evaluating and mitigating overfitting is essential for successful model development.\n",
    "\n",
    "We utilized hyperparameter tuning as a comprehensive approach to address overfitting. By adjusting these parameters, we have observed significant improvements in various model performance metrics. This includes a reduction in overfitting and an increase in generalization capabilities. The effectiveness of these enhancements is often achieved through established metrics such as accuracy, precision, recall, and F1-score. These metrics collectively reflected the model's ability to generate accurate predictions on both the training and unseen data.\n",
    "\n",
    "Visualization of model performance before and after the hyperparameter tuning process played a crucial role in determining the impact of these adjustments. Techniques such as ROC curves and confusion matrices visually illustrated the model's discriminatory ability and error patterns. These visualizations gave us valuable insights into how the tuning efforts have influenced the model's predictive ability and robustness.\n",
    "\n",
    "In conclusion, by rigorously evaluating and addressing overfitting through strategic hyperparameter tuning, we have created more reliable and effective machine learning models. This facilitates the creation of more accurate predictions and fosters informed decision-making processes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1deaa1",
   "metadata": {},
   "source": [
    "### **Evaluation of the imbalanced outcome issue**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb0a345",
   "metadata": {},
   "source": [
    "A significant issue arises when datasets exhibit class imbalance, where one class is significantly underrepresented compared to another. In medical diagnosis, for instance, positive cases of a rare disease may be significantly outnumbered by negative cases representing healthy individuals. This imbalance may generate models that are biased towards the majority class, potentially overlooking patterns associated with the minority class.\n",
    "\n",
    "The techniques we used to address class imbalance is oversampling and undersampling. Oversampling is intended to reduce the class distribution by increasing the number of instances in the minority class. This was achieved through techniques such as SMOTE (Synthetic Minority Over-sampling Technique), which generates synthetic data points to boost the minority class. On the other hand, undersampling reduces the size of the majority class by randomly removing data points until it aligns with the number of instances in the minority class.\n",
    "\n",
    "Both oversampling and undersampling techniques significantly enhanced model training. By balancing the class distribution, these methods alleviated bias towards the majority class, leading to improved performance metrics such as accuracy, precision, and recall. Furthermore, they enhanced model generalizability to the minority class, reducing the likelihood of overfitting to the majority class data.\n",
    "\n",
    "However, it is essential to acknowledge the potential consequences associated with the resampling techniques. Oversampling can lead to artificial biases or overfitting issues due to the generation of synthetic data points that are too similar to existing ones while undersampling disregards potentially valuable data from the majority class. Therefore, it is essential to analyze the trade-offs between model performance and data representativeness. The optimal approach will ultimately depend on the specific situation and objectives of the machine learning task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31e640d",
   "metadata": {},
   "source": [
    "### **Evaluation of classification error**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc399529",
   "metadata": {},
   "source": [
    "When evaluating classification errors in medical outcomes, it's crucial to differentiate between False Positive (Type I error) and False Negative (Type II error) occurrences. Several factors can be taken into consideration when assessing and judging these classification errors, namely patient care, costs, and stakeholder perspectives.\n",
    "\n",
    "One key consideration will be the impact on patient outcomes. It is essential to understand which error type is more risky to patient health and well-being. In addition to that we should consider the cost implications associated with the treatment. This involves comparing the costs associated with false positives, such as unnecessary treatments, versus false negatives, like delayed or missed treatments. Incorporating stakeholder perspectives, including input from medical professionals, patients, and healthcare administrators, provides a comprehensive view of the implications of classification errors in medical contexts.\n",
    "\n",
    "In terms of which error is more significant or costly, False Negatives (Type II Error) often take precedence in medical settings. This is because of the potentially higher consequences stemming from missed diagnoses and delayed treatments. While False Positives (Type I Error) can lead to unnecessary interventions, they may be deemed less harmful than missing a critical diagnosis or delaying crucial treatments that could significantly impact patient outcomes. Hence, prioritizing strategies to reduce False Negatives is usually the priority in medical decision-making and evaluation processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fe7b82",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4bbbc5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this research project, I employed four distinct models, each yielding varying results. KNN with SMOTE and neural networks demonstrated superior performance compared to the other models. However, XGBoosting and decision trees did not perform as well. I suspect that decision trees may have suffered from overfitting and data leakage issues. Despite my attempt to mitigate these issues by using SMOTE instead of bootstrapping, the results were not satisfactory. To address this, I propose requesting a holdout dataset from the client, which was not utilized during model training or hyperparameter tuning. This approach will allow to evaluate the model's performance on unseen data, providing a more realistic estimate of its generalization performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
